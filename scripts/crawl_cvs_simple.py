"""
3ëŒ€ í¸ì˜ì  Freshfood í¬ë¡¤ë§ (Requests ê¸°ë°˜)
- ì„¸ë¸ì¼ë ˆë¸, CU, GS25
- API ì§ì ‘ í˜¸ì¶œ ë°©ì‹
"""
import os
import json
import time
import requests
from pathlib import Path
from bs4 import BeautifulSoup
import re


# ì¶œë ¥ ë””ë ‰í† ë¦¬
OUTPUT_DIR = Path("/Users/js/Documents/stopper/data/convenience_crawl")
OUTPUT_DIR.mkdir(exist_ok=True)

IMAGES_DIR = OUTPUT_DIR / "images"
IMAGES_DIR.mkdir(exist_ok=True)


def download_image(url, save_path):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    try:
        if url.startswith('//'):
            url = 'https:' + url
        elif url.startswith('/'):
            return None

        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })

        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return str(save_path)
    except Exception as e:
        print(f"   ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url[:50]}... - {e}")
    return None


def crawl_7eleven():
    """ì„¸ë¸ì¼ë ˆë¸ freshfood í¬ë¡¤ë§ (AJAX API ì§ì ‘ í˜¸ì¶œ)"""
    print("\nğŸ” ì„¸ë¸ì¼ë ˆë¸ í¬ë¡¤ë§ ì‹œì‘...")

    products = []

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Referer': 'https://www.7-eleven.co.kr/product/bestdosirakList.asp'
        }

        # ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ
        response = requests.get(
            'https://www.7-eleven.co.kr/product/bestdosirakList.asp',
            headers=headers
        )

        soup = BeautifulSoup(response.text, 'html.parser')

        # ì œí’ˆ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
        product_items = soup.select('.wrap_list_02 li')
        print(f"   ì´ˆê¸° í˜ì´ì§€: {len(product_items)}ê°œ ì œí’ˆ ë°œê²¬")

        for item in product_items:
            try:
                img_elem = item.find('img')
                if not img_elem:
                    continue

                name = img_elem.get('alt', '').strip()
                img_url = img_elem.get('src', '')

                # ê°€ê²© ì¶”ì¶œ
                price = None
                text = item.get_text()
                if 'ì›' in text:
                    price_match = re.search(r'([\d,]+)ì›', text)
                    if price_match:
                        price = price_match.group(1).replace(',', '')

                if img_url.startswith('/'):
                    img_url = f"https://www.7-eleven.co.kr{img_url}"

                if name:
                    products.append({
                        "store": "7-ELEVEN",
                        "name": name,
                        "price": price,
                        "image_url": img_url
                    })

            except Exception as e:
                continue

        # MORE ë²„íŠ¼ AJAX í˜¸ì¶œ (í˜ì´ì§€ í™•ì¥)
        page_size = 4
        max_pages = 50

        for page in range(1, max_pages):
            try:
                ajax_url = "https://www.7-eleven.co.kr/product/dosirakNewMoreAjax.asp"
                ajax_data = {
                    'intPageSize': page_size * (page + 1),
                    'pTab': '1'
                }

                ajax_response = requests.post(ajax_url, data=ajax_data, headers=headers, timeout=10)

                if ajax_response.status_code != 200:
                    break

                ajax_soup = BeautifulSoup(ajax_response.text, 'html.parser')
                ajax_items = ajax_soup.select('li')

                if not ajax_items:
                    break

                print(f"   í˜ì´ì§€ {page + 1}: {len(ajax_items)}ê°œ ì¶”ê°€ ì œí’ˆ")

                for item in ajax_items:
                    try:
                        img_elem = item.find('img')
                        if not img_elem:
                            continue

                        name = img_elem.get('alt', '').strip()
                        img_url = img_elem.get('src', '')

                        price = None
                        text = item.get_text()
                        if 'ì›' in text:
                            price_match = re.search(r'([\d,]+)ì›', text)
                            if price_match:
                                price = price_match.group(1).replace(',', '')

                        if img_url.startswith('/'):
                            img_url = f"https://www.7-eleven.co.kr{img_url}"

                        if name and name not in [p['name'] for p in products]:
                            products.append({
                                "store": "7-ELEVEN",
                                "name": name,
                                "price": price,
                                "image_url": img_url
                            })

                    except Exception as e:
                        continue

                time.sleep(0.5)

            except Exception as e:
                print(f"   í˜ì´ì§€ {page + 1} ë¡œë“œ ì‹¤íŒ¨: {e}")
                break

        print(f"\nâœ… ì„¸ë¸ì¼ë ˆë¸: {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì„¸ë¸ì¼ë ˆë¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    return products


def crawl_cu():
    """CU freshfood í¬ë¡¤ë§ (AJAX API ì§ì ‘ í˜¸ì¶œ)"""
    print("\nğŸ” CU í¬ë¡¤ë§ ì‹œì‘...")

    products = []

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Referer': 'https://cu.bgfretail.com/product/product.do'
        }

        # depth3 ì¹´í…Œê³ ë¦¬ ìˆœíšŒ (1~10ê¹Œì§€ ì‹œë„)
        for depth3 in range(1, 11):
            try:
                print(f"\n   ì¹´í…Œê³ ë¦¬ depth3={depth3} í¬ë¡¤ë§...")

                # AJAX ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
                ajax_url = "https://cu.bgfretail.com/product/productAjax.do"

                for page in range(1, 20):
                    ajax_data = {
                        'pageIndex': page,
                        'listType': 'list',
                        'category': 'product',
                        'depth2': '4',
                        'depth3': str(depth3)
                    }

                    response = requests.post(ajax_url, data=ajax_data, headers=headers, timeout=10)

                    if response.status_code != 200:
                        break

                    soup = BeautifulSoup(response.text, 'html.parser')
                    items = soup.select('li')

                    if not items:
                        break

                    print(f"      í˜ì´ì§€ {page}: {len(items)}ê°œ ì œí’ˆ")

                    for item in items:
                        try:
                            # ì œí’ˆëª…
                            name_elem = item.select_one('.prodName, .prod_name, .name')
                            if not name_elem:
                                continue

                            name = name_elem.get_text().strip()

                            # ê°€ê²©
                            price = None
                            price_elem = item.select_one('.price em, .cost em, .price')
                            if price_elem:
                                price = price_elem.get_text().strip().replace(',', '').replace('ì›', '')

                            # ì´ë¯¸ì§€
                            img_elem = item.find('img')
                            img_url = None
                            if img_elem:
                                img_url = img_elem.get('src', '')
                                if img_url.startswith('/'):
                                    img_url = f"https://cu.bgfretail.com{img_url}"

                            if name and name not in [p['name'] for p in products]:
                                products.append({
                                    "store": "CU",
                                    "name": name,
                                    "price": price,
                                    "image_url": img_url
                                })

                        except Exception as e:
                            continue

                    time.sleep(0.3)

            except Exception as e:
                print(f"   ì¹´í…Œê³ ë¦¬ {depth3} ì˜¤ë¥˜: {e}")
                continue

        print(f"\nâœ… CU: {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ CU í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    return products


def crawl_gs25():
    """GS25 freshfood í¬ë¡¤ë§"""
    print("\nğŸ” GS25 í¬ë¡¤ë§ ì‹œì‘...")

    products = []

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Referer': 'http://gs25.gsretail.com/gscvs/ko/products/youus-freshfood'
        }

        # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì¡°íšŒ
        response = requests.get(
            'http://gs25.gsretail.com/gscvs/ko/products/youus-freshfood',
            headers=headers,
            timeout=10
        )

        soup = BeautifulSoup(response.text, 'html.parser')

        # ì œí’ˆ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
        items = soup.select('.prod_box, .product-list-item, .prod_list li')
        print(f"   {len(items)}ê°œ ì œí’ˆ ë°œê²¬")

        for item in items:
            try:
                # ì œí’ˆëª…
                name_elem = item.select_one('.tit, .prod_name, .name')
                if not name_elem:
                    continue

                name = name_elem.get_text().strip()

                # ê°€ê²©
                price = None
                price_elem = item.select_one('.price, .cost em, .cost')
                if price_elem:
                    price = price_elem.get_text().strip().replace(',', '').replace('ì›', '')

                # ì´ë¯¸ì§€
                img_elem = item.find('img')
                img_url = None
                if img_elem:
                    img_url = img_elem.get('src', '')
                    if img_url.startswith('/'):
                        img_url = f"http://gs25.gsretail.com{img_url}"

                if name:
                    products.append({
                        "store": "GS25",
                        "name": name,
                        "price": price,
                        "image_url": img_url
                    })

            except Exception as e:
                continue

        print(f"\nâœ… GS25: {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ GS25 í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    return products


def download_all_images(products):
    """ìˆ˜ì§‘í•œ ì œí’ˆì˜ ì´ë¯¸ì§€ ì¼ê´„ ë‹¤ìš´ë¡œë“œ"""
    print("\nğŸ“¥ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

    downloaded = 0
    failed = 0

    for idx, product in enumerate(products, 1):
        img_url = product.get('image_url')
        if not img_url:
            continue

        # íŒŒì¼ëª…: store_ì œí’ˆëª…ì²˜ìŒ10ì_index.jpg
        store = product['store'].lower().replace('-', '')
        safe_name = re.sub(r'[^\wê°€-í£]', '', product['name'][:10])
        filename = f"{store}_{safe_name}_{idx:04d}.jpg"
        save_path = IMAGES_DIR / filename

        if download_image(img_url, save_path):
            product['image_path'] = filename  # ìƒëŒ€ ê²½ë¡œë§Œ ì €ì¥
            downloaded += 1
        else:
            failed += 1

        if idx % 20 == 0:
            print(f"   ì§„í–‰: {idx}/{len(products)} (ì„±ê³µ: {downloaded}, ì‹¤íŒ¨: {failed})")

    print(f"\nâœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("ğŸª 3ëŒ€ í¸ì˜ì  Freshfood í¬ë¡¤ë§")
    print("="*60)

    all_products = []

    # 1. ì„¸ë¸ì¼ë ˆë¸
    products_7eleven = crawl_7eleven()
    all_products.extend(products_7eleven)

    # 2. CU
    products_cu = crawl_cu()
    all_products.extend(products_cu)

    # 3. GS25
    products_gs25 = crawl_gs25()
    all_products.extend(products_gs25)

    print("\n" + "="*60)
    print(f"ğŸ“Š ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼")
    print("="*60)
    print(f"ì„¸ë¸ì¼ë ˆë¸: {len(products_7eleven)}ê°œ")
    print(f"CU: {len(products_cu)}ê°œ")
    print(f"GS25: {len(products_gs25)}ê°œ")
    print(f"ì´í•©: {len(all_products)}ê°œ")

    # JSON ì €ì¥
    output_file = OUTPUT_DIR / "convenience_products_all.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥: {output_file}")

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    download_all_images(all_products)

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í›„ ìµœì¢… JSON ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   ë°ì´í„°: {output_file}")
    print(f"   ì´ë¯¸ì§€: {IMAGES_DIR}")


if __name__ == "__main__":
    main()
