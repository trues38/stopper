"""
3ëŒ€ í¸ì˜ì  Freshfood í¬ë¡¤ë§
- ì„¸ë¸ì¼ë ˆë¸, CU, GS25
- ì œí’ˆëª…, ê°€ê²©, ì´ë¯¸ì§€ URL ìˆ˜ì§‘ ë° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
"""
import os
import json
import time
import requests
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException


# ì¶œë ¥ ë””ë ‰í† ë¦¬
OUTPUT_DIR = Path("/Users/js/Documents/stopper/data/convenience_crawl")
OUTPUT_DIR.mkdir(exist_ok=True)

IMAGES_DIR = OUTPUT_DIR / "images"
IMAGES_DIR.mkdir(exist_ok=True)


def init_driver(headless=True):
    """Chrome driver ì´ˆê¸°í™”"""
    chrome_options = Options()
    if headless:
        chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36")

    driver = webdriver.Chrome(options=chrome_options)
    return driver


def download_image(url, save_path):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    try:
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if url.startswith('/'):
            # ê° ì‚¬ì´íŠ¸ì˜ base URL ì¶”ê°€ í•„ìš”
            return None

        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return str(save_path)
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url} - {e}")
    return None


def crawl_7eleven():
    """ì„¸ë¸ì¼ë ˆë¸ freshfood í¬ë¡¤ë§"""
    print("\nğŸ” ì„¸ë¸ì¼ë ˆë¸ í¬ë¡¤ë§ ì‹œì‘...")

    driver = init_driver(headless=False)
    products = []

    try:
        url = "https://www.7-eleven.co.kr/product/bestdosirakList.asp"
        driver.get(url)

        # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        time.sleep(2)

        # MORE ë²„íŠ¼ ë°˜ë³µ í´ë¦­
        click_count = 0
        max_clicks = 50  # ìµœëŒ€ í´ë¦­ íšŸìˆ˜

        while click_count < max_clicks:
            try:
                # MORE ë²„íŠ¼ ì°¾ê¸°
                more_btn = driver.find_element(By.CLASS_NAME, "btn_more")

                # ë²„íŠ¼ì´ ìˆ¨ê²¨ì ¸ ìˆìœ¼ë©´ ì¤‘ë‹¨
                if "none" in more_btn.get_attribute("style") or not more_btn.is_displayed():
                    print("   ë” ì´ìƒ ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
                    break

                # ë²„íŠ¼ í´ë¦­
                driver.execute_script("arguments[0].click();", more_btn)
                click_count += 1
                print(f"   MORE ë²„íŠ¼ í´ë¦­: {click_count}íšŒ")

                # ë¡œë”© ëŒ€ê¸°
                time.sleep(1.5)

            except NoSuchElementException:
                print("   MORE ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"   MORE ë²„íŠ¼ í´ë¦­ ì˜¤ë¥˜: {e}")
                break

        # ì „ì²´ ì œí’ˆ ìˆ˜ì§‘
        print("\n   ì œí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

        product_items = driver.find_elements(By.CSS_SELECTOR, ".wrap_list_02 li")
        print(f"   ì´ {len(product_items)}ê°œ ì œí’ˆ ë°œê²¬")

        for idx, item in enumerate(product_items, 1):
            try:
                # ì´ë¯¸ì§€
                img_elem = item.find_element(By.TAG_NAME, "img")
                img_url = img_elem.get_attribute("src")

                # ì œí’ˆëª… (alt ì†ì„± ë˜ëŠ” title)
                name = img_elem.get_attribute("alt") or img_elem.get_attribute("title") or ""
                name = name.strip()

                # ê°€ê²© ì¶”ì¶œ (ì´ë¯¸ì§€ ì˜† í…ìŠ¤íŠ¸ì—ì„œ)
                try:
                    # ê°€ê²©ì€ ë³´í†µ "ì›" ì•ì˜ ìˆ«ì
                    price_text = item.text
                    if "ì›" in price_text:
                        price = price_text.split("ì›")[0].strip().split()[-1]
                        price = price.replace(",", "")
                    else:
                        price = None
                except:
                    price = None

                if not name:
                    continue

                # ì´ë¯¸ì§€ URL ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
                if img_url.startswith('/'):
                    img_url = f"https://www.7-eleven.co.kr{img_url}"

                product = {
                    "store": "7-ELEVEN",
                    "name": name,
                    "price": price,
                    "image_url": img_url
                }

                products.append(product)

                if idx % 10 == 0:
                    print(f"   ì§„í–‰: {idx}/{len(product_items)}")

            except Exception as e:
                print(f"   ì œí’ˆ {idx} íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        print(f"\nâœ… ì„¸ë¸ì¼ë ˆë¸: {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì„¸ë¸ì¼ë ˆë¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    finally:
        driver.quit()

    return products


def crawl_cu():
    """CU freshfood í¬ë¡¤ë§"""
    print("\nğŸ” CU í¬ë¡¤ë§ ì‹œì‘...")

    driver = init_driver(headless=False)
    products = []

    try:
        # depth3=1ë¶€í„° ì‹œì‘í•´ì„œ ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        categories = [1, 2, 3, 4, 5]  # freshfood í•˜ìœ„ ì¹´í…Œê³ ë¦¬

        for depth3 in categories:
            url = f"https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3={depth3}"
            print(f"\n   ì¹´í…Œê³ ë¦¬ {depth3} í¬ë¡¤ë§...")

            driver.get(url)
            time.sleep(2)

            # ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ ëª¨ë“  ì œí’ˆ ë¡œë“œ
            last_height = driver.execute_script("return document.body.scrollHeight")
            scroll_count = 0
            max_scrolls = 10

            while scroll_count < max_scrolls:
                # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1.5)

                # ìƒˆë¡œìš´ ë†’ì´ ê³„ì‚°
                new_height = driver.execute_script("return document.body.scrollHeight")

                # ë” ì´ìƒ ë¡œë“œë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨
                if new_height == last_height:
                    break

                last_height = new_height
                scroll_count += 1
                print(f"      ìŠ¤í¬ë¡¤: {scroll_count}íšŒ")

            # ì œí’ˆ ìˆ˜ì§‘
            try:
                product_items = driver.find_elements(By.CSS_SELECTOR, "#dataTable .prodListWrap li")
                print(f"      {len(product_items)}ê°œ ì œí’ˆ ë°œê²¬")

                for item in product_items:
                    try:
                        # ì œí’ˆëª…
                        name_elem = item.find_element(By.CSS_SELECTOR, ".prodName")
                        name = name_elem.text.strip()

                        # ê°€ê²©
                        try:
                            price_elem = item.find_element(By.CSS_SELECTOR, ".price em")
                            price = price_elem.text.strip().replace(",", "")
                        except:
                            price = None

                        # ì´ë¯¸ì§€
                        try:
                            img_elem = item.find_element(By.TAG_NAME, "img")
                            img_url = img_elem.get_attribute("src")

                            # ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
                            if img_url and img_url.startswith('/'):
                                img_url = f"https://cu.bgfretail.com{img_url}"
                        except:
                            img_url = None

                        if name:
                            product = {
                                "store": "CU",
                                "name": name,
                                "price": price,
                                "image_url": img_url
                            }
                            products.append(product)

                    except Exception as e:
                        continue

            except NoSuchElementException:
                print(f"      ì¹´í…Œê³ ë¦¬ {depth3}ì— ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
                continue

        print(f"\nâœ… CU: {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ CU í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    finally:
        driver.quit()

    return products


def crawl_gs25():
    """GS25 freshfood í¬ë¡¤ë§"""
    print("\nğŸ” GS25 í¬ë¡¤ë§ ì‹œì‘...")

    driver = init_driver(headless=False)
    products = []

    try:
        url = "http://gs25.gsretail.com/gscvs/ko/products/youus-freshfood"
        driver.get(url)

        # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        time.sleep(3)

        # ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ ëª¨ë“  ì œí’ˆ ë¡œë“œ
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        max_scrolls = 20

        while scroll_count < max_scrolls:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

            new_height = driver.execute_script("return document.body.scrollHeight")

            if new_height == last_height:
                break

            last_height = new_height
            scroll_count += 1
            print(f"   ìŠ¤í¬ë¡¤: {scroll_count}íšŒ")

        # "ë”ë³´ê¸°" ë²„íŠ¼ í´ë¦­ ì‹œë„
        try:
            while True:
                more_btn = driver.find_element(By.CSS_SELECTOR, ".btn_more, .mCSB_buttonDown")
                if more_btn.is_displayed():
                    driver.execute_script("arguments[0].click();", more_btn)
                    time.sleep(2)
                else:
                    break
        except:
            pass

        # ì œí’ˆ ìˆ˜ì§‘
        product_items = driver.find_elements(By.CSS_SELECTOR, ".prod_list li, .product-list-item")
        print(f"   {len(product_items)}ê°œ ì œí’ˆ ë°œê²¬")

        for item in product_items:
            try:
                # ì œí’ˆëª…
                name_elem = item.find_element(By.CSS_SELECTOR, ".tit, .prod_name, .name")
                name = name_elem.text.strip()

                # ê°€ê²©
                try:
                    price_elem = item.find_element(By.CSS_SELECTOR, ".price, .cost em")
                    price = price_elem.text.strip().replace(",", "").replace("ì›", "")
                except:
                    price = None

                # ì´ë¯¸ì§€
                try:
                    img_elem = item.find_element(By.TAG_NAME, "img")
                    img_url = img_elem.get_attribute("src")

                    if img_url and img_url.startswith('/'):
                        img_url = f"http://gs25.gsretail.com{img_url}"
                except:
                    img_url = None

                if name:
                    product = {
                        "store": "GS25",
                        "name": name,
                        "price": price,
                        "image_url": img_url
                    }
                    products.append(product)

            except Exception as e:
                continue

        print(f"\nâœ… GS25: {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ GS25 í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    finally:
        driver.quit()

    return products


def download_all_images(products):
    """ìˆ˜ì§‘í•œ ì œí’ˆì˜ ì´ë¯¸ì§€ ì¼ê´„ ë‹¤ìš´ë¡œë“œ"""
    print("\nğŸ“¥ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

    downloaded = 0
    failed = 0

    for idx, product in enumerate(products, 1):
        img_url = product.get('image_url')
        if not img_url:
            continue

        # íŒŒì¼ëª…: store_index.jpg
        store = product['store'].lower().replace('-', '')
        filename = f"{store}_{idx:04d}.jpg"
        save_path = IMAGES_DIR / filename

        if download_image(img_url, save_path):
            product['image_path'] = str(save_path)
            downloaded += 1
        else:
            failed += 1

        if idx % 50 == 0:
            print(f"   ì§„í–‰: {idx}/{len(products)} (ì„±ê³µ: {downloaded}, ì‹¤íŒ¨: {failed})")

    print(f"\nâœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("ğŸª 3ëŒ€ í¸ì˜ì  Freshfood í¬ë¡¤ë§")
    print("="*60)

    all_products = []

    # 1. ì„¸ë¸ì¼ë ˆë¸
    products_7eleven = crawl_7eleven()
    all_products.extend(products_7eleven)

    # 2. CU
    products_cu = crawl_cu()
    all_products.extend(products_cu)

    # 3. GS25
    products_gs25 = crawl_gs25()
    all_products.extend(products_gs25)

    print("\n" + "="*60)
    print(f"ğŸ“Š ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼")
    print("="*60)
    print(f"ì„¸ë¸ì¼ë ˆë¸: {len(products_7eleven)}ê°œ")
    print(f"CU: {len(products_cu)}ê°œ")
    print(f"GS25: {len(products_gs25)}ê°œ")
    print(f"ì´í•©: {len(all_products)}ê°œ")

    # JSON ì €ì¥
    output_file = OUTPUT_DIR / "convenience_products_all.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥: {output_file}")

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    download_all_images(all_products)

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í›„ ìµœì¢… JSON ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   ë°ì´í„°: {output_file}")
    print(f"   ì´ë¯¸ì§€: {IMAGES_DIR}")


if __name__ == "__main__":
    main()
