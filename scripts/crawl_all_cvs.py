"""
3ëŒ€ í¸ì˜ì  Freshfood í¬ë¡¤ë§ (ê°œì„  ë²„ì „)
- 7-Eleven: Selenium + JavaScript ë³€ìˆ˜ ì¶”ì¶œ
- CU: Requests + AJAX API
- GS25: Selenium + HTML íŒŒì‹±
"""
import os
import json
import time
import re
import requests
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup


# ì¶œë ¥ ë””ë ‰í† ë¦¬
OUTPUT_DIR = Path("/Users/js/Documents/stopper/data/convenience_crawl")
OUTPUT_DIR.mkdir(exist_ok=True)

IMAGES_DIR = OUTPUT_DIR / "images"
IMAGES_DIR.mkdir(exist_ok=True)


def init_driver(headless=False):
    """Chrome driver ì´ˆê¸°í™”"""
    chrome_options = Options()
    if headless:
        chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36")

    driver = webdriver.Chrome(options=chrome_options)
    return driver


def download_image(url, save_path):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    try:
        # Fix double slash in URL
        if '//' in url[8:]:
            url = url.replace('cu.bgfretail.com//', '')
            if not url.startswith('http'):
                url = 'https://' + url

        if url.startswith('//'):
            url = 'https:' + url
        elif url.startswith('/'):
            # Relative URL - skip for now
            return None

        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0'
        })

        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return str(save_path.name)
    except Exception as e:
        print(f"   ì´ë¯¸ì§€ ì‹¤íŒ¨: {url[:50]}...")
    return None


def crawl_7eleven():
    """7-Eleven - Selenium + JavaScript ë³€ìˆ˜ ì¶”ì¶œ"""
    print("\nğŸ” 7-Eleven í¬ë¡¤ë§ ì‹œì‘...")

    driver = init_driver(headless=False)
    products = []

    try:
        url = "https://www.7-eleven.co.kr/product/bestdosirakList.asp"
        driver.get(url)
        time.sleep(2)

        # MORE ë²„íŠ¼ í´ë¦­í•´ì„œ ì „ì²´ ë¡œë“œ
        click_count = 0
        max_clicks = 50

        while click_count < max_clicks:
            try:
                more_btn = driver.find_element(By.CLASS_NAME, "btn_more")

                if "none" in more_btn.get_attribute("style") or not more_btn.is_displayed():
                    print(f"   ë” ì´ìƒ ì œí’ˆ ì—†ìŒ (í´ë¦­ {click_count}íšŒ)")
                    break

                driver.execute_script("arguments[0].click();", more_btn)
                click_count += 1

                if click_count % 10 == 0:
                    print(f"   MORE ë²„íŠ¼ í´ë¦­: {click_count}íšŒ")

                time.sleep(1)

            except NoSuchElementException:
                print(f"   MORE ë²„íŠ¼ ì—†ìŒ (í´ë¦­ {click_count}íšŒ)")
                break
            except Exception as e:
                print(f"   í´ë¦­ ì¤‘ë‹¨: {e}")
                break

        # JavaScript ë³€ìˆ˜ galleryArray ì¶”ì¶œ
        page_source = driver.page_source
        match = re.search(r'galleryArray\s*=\s*\[(.*?)\];', page_source, re.DOTALL)

        if match:
            array_content = match.group(1)
            # ê°œë³„ ê°ì²´ ì¶”ì¶œ
            objects = re.findall(r'\{([^}]+)\}', array_content)
            print(f"   galleryArray: {len(objects)}ê°œ ì œí’ˆ ë°œê²¬")

            for obj_str in objects:
                # í•„ë“œ íŒŒì‹±
                name_match = re.search(r"alt:\s*'([^']+)'", obj_str)
                price_match = re.search(r"price:\s*'([^']+)'", obj_str)
                img_match = re.search(r"src:\s*'([^']+)'", obj_str)

                if name_match:
                    name = name_match.group(1)
                    price = price_match.group(1).replace(',', '') if price_match else None
                    img_url = img_match.group(1) if img_match else None

                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ
                    if img_url and img_url.startswith('/'):
                        img_url = f"https://www.7-eleven.co.kr{img_url}"

                    products.append({
                        "store": "7-ELEVEN",
                        "name": name,
                        "price": price,
                        "image_url": img_url
                    })

        print(f"\nâœ… 7-Eleven: {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ 7-Eleven í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    finally:
        driver.quit()

    return products


def crawl_cu():
    """CU - Requests + AJAX API (ê¸°ì¡´ ë°©ì‹)"""
    print("\nğŸ” CU í¬ë¡¤ë§ ì‹œì‘...")

    products = []

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Referer': 'https://cu.bgfretail.com/product/product.do'
        }

        for depth3 in range(1, 11):
            print(f"\n   ì¹´í…Œê³ ë¦¬ depth3={depth3} í¬ë¡¤ë§...")

            ajax_url = "https://cu.bgfretail.com/product/productAjax.do"

            for page in range(1, 20):
                ajax_data = {
                    'pageIndex': page,
                    'listType': 'list',
                    'category': 'product',
                    'depth2': '4',
                    'depth3': str(depth3)
                }

                response = requests.post(ajax_url, data=ajax_data, headers=headers, timeout=10)

                if response.status_code != 200:
                    break

                soup = BeautifulSoup(response.text, 'html.parser')
                items = soup.select('li')

                if not items:
                    break

                if page == 1:
                    print(f"      í˜ì´ì§€ {page}: {len(items)}ê°œ ì œí’ˆ")

                for item in items:
                    try:
                        name_elem = item.select_one('.prodName, .prod_name, .name')
                        if not name_elem:
                            continue

                        name = name_elem.get_text().strip()

                        price = None
                        price_elem = item.select_one('.price em, .cost em, .price')
                        if price_elem:
                            price = price_elem.get_text().strip().replace(',', '').replace('ì›', '')

                        img_elem = item.find('img')
                        img_url = None
                        if img_elem:
                            img_url = img_elem.get('src', '')
                            if img_url.startswith('/'):
                                img_url = f"https://cu.bgfretail.com{img_url}"

                        if name and name not in [p['name'] for p in products]:
                            products.append({
                                "store": "CU",
                                "name": name,
                                "price": price,
                                "image_url": img_url
                            })

                    except Exception:
                        continue

                time.sleep(0.3)

        print(f"\nâœ… CU: {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ CU í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    return products


def crawl_gs25():
    """GS25 - Selenium + HTML íŒŒì‹±"""
    print("\nğŸ” GS25 í¬ë¡¤ë§ ì‹œì‘...")

    driver = init_driver(headless=False)
    products = []

    try:
        url = "http://gs25.gsretail.com/gscvs/ko/products/youus-freshfood"
        driver.get(url)
        time.sleep(3)

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´ìœ¼ë¡œ ì œí’ˆ ë¡œë“œ
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        max_scrolls = 30

        while scroll_count < max_scrolls:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

            new_height = driver.execute_script("return document.body.scrollHeight")

            if new_height == last_height:
                break

            last_height = new_height
            scroll_count += 1

            if scroll_count % 5 == 0:
                print(f"   ìŠ¤í¬ë¡¤: {scroll_count}íšŒ")

        # í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ì œí’ˆ ì¶”ì¶œ
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
        selectors = [
            '.prod_box',
            '.product-list-item',
            '.prod_list li',
            'div.prod',
            'li[class*="prod"]'
        ]

        items = []
        for selector in selectors:
            items = soup.select(selector)
            if items:
                print(f"   ì…€ë ‰í„° '{selector}': {len(items)}ê°œ ë°œê²¬")
                break

        if not items:
            # li íƒœê·¸ ì „ì²´ ì¤‘ì—ì„œ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²ƒë§Œ
            all_lis = soup.find_all('li')
            items = [li for li in all_lis if li.find('img', src=True)]
            print(f"   ì´ë¯¸ì§€ í¬í•¨ LI: {len(items)}ê°œ")

        for item in items:
            try:
                # ì œí’ˆëª…
                name_elem = item.select_one('.tit, .prod_name, .name, .prodName')
                if not name_elem:
                    # alt ì†ì„±ì—ì„œ ì°¾ê¸°
                    img = item.find('img')
                    if img:
                        name = img.get('alt', '').strip()
                    else:
                        continue
                else:
                    name = name_elem.get_text().strip()

                if not name or len(name) < 3:
                    continue

                # ê°€ê²©
                price = None
                price_elem = item.select_one('.price, .cost em, .cost')
                if price_elem:
                    price = price_elem.get_text().strip().replace(',', '').replace('ì›', '')

                # ì´ë¯¸ì§€
                img_elem = item.find('img')
                img_url = None
                if img_elem:
                    img_url = img_elem.get('src', '')
                    if img_url.startswith('/'):
                        img_url = f"http://gs25.gsretail.com{img_url}"

                if name:
                    products.append({
                        "store": "GS25",
                        "name": name,
                        "price": price,
                        "image_url": img_url
                    })

            except Exception:
                continue

        print(f"\nâœ… GS25: {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ GS25 í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    finally:
        driver.quit()

    return products


def download_all_images(products):
    """ì´ë¯¸ì§€ ì¼ê´„ ë‹¤ìš´ë¡œë“œ"""
    print("\nğŸ“¥ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

    downloaded = 0
    failed = 0

    for idx, product in enumerate(products, 1):
        img_url = product.get('image_url')
        if not img_url:
            failed += 1
            continue

        # íŒŒì¼ëª…: store_ì œí’ˆëª…ì²˜ìŒ10ì_index.jpg
        store = product['store'].lower().replace('-', '')
        safe_name = re.sub(r'[^\wê°€-í£]', '', product['name'][:10])
        filename = f"{store}_{safe_name}_{idx:04d}.jpg"
        save_path = IMAGES_DIR / filename

        result = download_image(img_url, save_path)
        if result:
            product['image_file'] = result
            downloaded += 1
        else:
            failed += 1

        if idx % 50 == 0:
            print(f"   ì§„í–‰: {idx}/{len(products)} (ì„±ê³µ: {downloaded}, ì‹¤íŒ¨: {failed})")

    print(f"\nâœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("ğŸª 3ëŒ€ í¸ì˜ì  Freshfood í¬ë¡¤ë§")
    print("="*60)

    all_products = []

    # 1. 7-Eleven
    products_7eleven = crawl_7eleven()
    all_products.extend(products_7eleven)

    # 2. CU
    products_cu = crawl_cu()
    all_products.extend(products_cu)

    # 3. GS25
    products_gs25 = crawl_gs25()
    all_products.extend(products_gs25)

    print("\n" + "="*60)
    print(f"ğŸ“Š ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼")
    print("="*60)
    print(f"7-Eleven: {len(products_7eleven)}ê°œ")
    print(f"CU: {len(products_cu)}ê°œ")
    print(f"GS25: {len(products_gs25)}ê°œ")
    print(f"ì´í•©: {len(all_products)}ê°œ")

    # JSON ì €ì¥ (ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì „)
    output_file = OUTPUT_DIR / "convenience_products_all.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥: {output_file}")

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    download_all_images(all_products)

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í›„ ìµœì¢… JSON ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   ë°ì´í„°: {output_file}")
    print(f"   ì´ë¯¸ì§€: {IMAGES_DIR}")


if __name__ == "__main__":
    main()
